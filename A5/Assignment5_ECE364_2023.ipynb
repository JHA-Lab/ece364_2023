{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E32UBMT7VKMm"
   },
   "source": [
    "## Prepare python environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3015,
     "status": "ok",
     "timestamp": 1670732264390,
     "user": {
      "displayName": "Hongjie Wang",
      "userId": "16591498052837554386"
     },
     "user_tz": 300
    },
    "id": "y_lm7Q-9VKMn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1670732265544,
     "user": {
      "displayName": "Hongjie Wang",
      "userId": "16591498052837554386"
     },
     "user_tz": 300
    },
    "id": "ryOZJYQa3PG0"
   },
   "outputs": [],
   "source": [
    "random_state = 5 # use this to control randomness across runs e.g., dataset partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the names of column in the dataset. It includes all features of the data and the label.\n",
    "col_names = ['pregnancies', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "import os\n",
    "if not os.path.exists(\"diabetes.csv\"):\n",
    "    !wget https://raw.githubusercontent.com/JHA-Lab/ece364_2023/master/dataset/diabetes.csv \n",
    "diabetes_data = pd.read_csv(\"diabetes.csv\", header=1, names=col_names)\n",
    "\n",
    "# Display the first five instances in the dataset\n",
    "diabetes_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract target and descriptive features (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into features and target variable\n",
    "X = # TODO\n",
    "y = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy array\n",
    "X = # TODO\n",
    "y = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and validation datasets (1 point)\n",
    "\n",
    "Split the data into training and validation sets using `train_test_split`.  See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set `random_state` to the value defined earlier. We use 80% of the data for training and 20% of the data for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset (1 point)\n",
    "\n",
    "#### Preprocess the data by normalizing each feature to have zero mean and unit standard deviation. This can be done using the `StandardScaler()` function. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler for scaling the data\n",
    "scaler = # TODO\n",
    "\n",
    "# Normalize the training data\n",
    "X_train = # TODO\n",
    "\n",
    "# Use the scaler defined above to standardize the validation data by applying the same transformation to the validation data.\n",
    "X_val = # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UtsyLvUNggX"
   },
   "source": [
    "## Training a Multi-Layer Perceptron (18 points)\n",
    "\n",
    "\n",
    "#### We will use `sklearn's` neural network library to train a multi-layer perceptron for classification. The model is trained to optimize the cross-entropy loss using stochastic gradient descent. Review ch.8 and see [here](https://scikit-learn.org/stable/modules/neural_networks_supervised.html) for more details. \n",
    "\n",
    "\n",
    "#### NOTE: Training each network takes several seconds to minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1670732293013,
     "user": {
      "displayName": "Hongjie Wang",
      "userId": "16591498052837554386"
     },
     "user_tz": 300
    },
    "id": "Frq2-KnMNf68"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import accuracy_score \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1670732294297,
     "user": {
      "displayName": "Hongjie Wang",
      "userId": "16591498052837554386"
     },
     "user_tz": 300
    },
    "id": "TWjR7AYWS80N"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For info on the arguments and attributes, see here: \n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)\n",
    "\"\"\"\n",
    "\n",
    "def get_mlp(hidden_layer_sizes=(100,),\n",
    "            activation='relu',\n",
    "            learning_rate_init=0.1,\n",
    "            early_stopping=False, \n",
    "            validation_fraction=0.15):\n",
    "  \n",
    "  # use stochastic gradient descent\n",
    "    parameters={'solver':'sgd',\n",
    "              'alpha': 0,\n",
    "              'momentum': 0,\n",
    "              'max_iter':20000,\n",
    "              'n_iter_no_change':100,\n",
    "              'tol': 1e-5,\n",
    "              'random_state': random_state\n",
    "              }\n",
    "    parameters['hidden_layer_sizes']=hidden_layer_sizes\n",
    "    parameters['activation']=activation\n",
    "    parameters['learning_rate_init']=learning_rate_init\n",
    "    parameters['early_stopping']=early_stopping\n",
    "    parameters['validation_fraction']=validation_fraction \n",
    "    \n",
    "    return MLPClassifier(**parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB9FECm31CHX"
   },
   "source": [
    "### Exercise 1: Warm up (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt7jsRvNPGRc"
   },
   "source": [
    "#### Use `get_mlp` defined above to create a multi-layer perceptron with 1 hidden layer consisting of 100 units and train the classifier on the training dataset. Keep all other parameters at their default values.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47430,
     "status": "ok",
     "timestamp": 1670532880110,
     "user": {
      "displayName": "Hongjie Wang",
      "userId": "16591498052837554386"
     },
     "user_tz": 300
    },
    "id": "GU2VrEfyO78v",
    "outputId": "7f03d174-c019-4d5e-f843-3be67dce05b8"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVCSowjdl43b"
   },
   "source": [
    "#### Visualize the evolution of the training loss. Hint: use `loss_curve_` attribute of the classifier and matplotlib (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1670532884726,
     "user": {
      "displayName": "Hongjie Wang",
      "userId": "16591498052837554386"
     },
     "user_tz": 300
    },
    "id": "BE_rJHewkua8",
    "outputId": "b0e45f7f-ac2f-4fe9-9c68-8bfa78d6a3f4"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xC3znkTUlFT0"
   },
   "source": [
    "#### Report the classifier's accuracies over the training and validation datasets. Hint: use `accuracy_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1670532890760,
     "user": {
      "displayName": "Hongjie Wang",
      "userId": "16591498052837554386"
     },
     "user_tz": 300
    },
    "id": "YVxvkqcoeSsE",
    "outputId": "bbab8390-56ec-4189-9625-f122c6aeef8b"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyOZ9NrE0U3g"
   },
   "source": [
    "#### Explain any performance difference observed between the training and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRwVEBpL0dSP"
   },
   "source": [
    "Insert answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFPaPLq90i36"
   },
   "source": [
    "#### We will next explore several strategies to improve the model's validation performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TUxVUfH1owa"
   },
   "source": [
    "### Exercise 2: Width vs Depth (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQKSzqClpIPy"
   },
   "source": [
    "#### Exercise 2a (4 points)\n",
    "\n",
    "#### Next, we will experiment with the width of the hidden layer, defined by the number of units in the hidden layer. \n",
    "\n",
    "#### Do this by using `get_mlp` to create a multi-layer perceptron with 1 hidden layer. Vary the number of hidden units among 1, 10, 50, 100, by setting `hidden_layer_sizes`. Keep all other parameters at their default values.\n",
    "\n",
    "#### Fit each classifier on the training dataset and report its training and validation accuracies.\n",
    " \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100906,
     "status": "ok",
     "timestamp": 1670362768898,
     "user": {
      "displayName": "Alexander La Cour",
      "userId": "15991973518995942312"
     },
     "user_tz": 300
    },
    "id": "-qJ836H4mf18",
    "outputId": "4c31b0c3-7437-4409-d663-fdf9877eb352"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfinbMwFsuhJ"
   },
   "source": [
    "#### Provide a possible explanation for any effect observed upon increasing the number of hidden units on classifier performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2e1eRd7s7Co"
   },
   "source": [
    "Insert answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yZ1OTk-2IXO"
   },
   "source": [
    "#### Exercise 2b (4 points)\n",
    "\n",
    "#### Next, we will experiment with the depth of the MLP, by varying the number of hidden layers. \n",
    "\n",
    "#### Do this by using `get_mlp` to create a Multi-layer perceptron with 15 units per hidden layer. Vary the number of hidden layers from 1 through 4, by setting `hidden_layer_sizes`. Keep all other parameters at their default values.\n",
    "\n",
    "#### Fit each classifier on the training dataset and report its training and validation accuracies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50403,
     "status": "ok",
     "timestamp": 1670362819284,
     "user": {
      "displayName": "Alexander La Cour",
      "userId": "15991973518995942312"
     },
     "user_tz": 300
    },
    "id": "gslpVDRU9ML3",
    "outputId": "9db44449-72bd-45a1-c2d2-4c5f75812d62"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ1T_Ap38oni"
   },
   "source": [
    "#### Provide a possible explanation for any change in performance upon increasing the model depth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02X85HZG9JHk"
   },
   "source": [
    "Insert answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx_K2IPdHYbG"
   },
   "source": [
    "#### Exercise 2c (4 points)\n",
    "\n",
    "#### Next, we'll explore the role of the hidden activation function when training a deeper network.\n",
    "\n",
    "#### Do this by using `get_mlp` to create a multi-layer perceptron with 5 hidden layers, each with 15 hidden units. Vary the activation functions among identity, logistic, tanh, and relu. Keep all other parameters at their default values.\n",
    "\n",
    "#### Fit each classifier on the training dataset and report its training accuracy.\n",
    "\n",
    "#### Also, plot the training loss curves for each classifier on a single plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "executionInfo": {
     "elapsed": 15756,
     "status": "ok",
     "timestamp": 1670733079387,
     "user": {
      "displayName": "Hongjie Wang",
      "userId": "16591498052837554386"
     },
     "user_tz": 300
    },
    "id": "uJVeur2DGv4j",
    "outputId": "0ef7be15-9e39-4368-94b0-1fb3f7dee055"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4W7fxGZ3LlUX"
   },
   "source": [
    "#### Explain any effect observed on the traininig loss trajectories and accuracies when varying the hidden activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uITvkxnuMb7k"
   },
   "source": [
    "Insert answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLkDAwEp1PS-"
   },
   "source": [
    "### Exercise 3: Early stopping (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9XgM0FP6hou"
   },
   "source": [
    "#### As we've seen from the above exercises, neural networks are prone to overfitting. To mitigate this, we can use a regularization method called early stopping. \n",
    "\n",
    "#### In this part, we will compare the performance of the model with the early stopping method and the one without the early stopping method. For fair comparison, we use the validation dataset built before (20% of the data) as test dataset, and we make it unavailable to both models until finally evaluating models on it. During training both models, we assume there is only the built training dataset (80% of the data) available.\n",
    "\n",
    "#### In early stopping, one monitors the performance of the model on a validation dataset (which is separated from the training dataset) throughout training. Then, the model with the lowest loss on the validation dataset, typically found in the earlier iterations of training, is selected, rather than the model with the lowest training loss. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzWFyWmrP7ic"
   },
   "source": [
    "#### Do this by calling `get_mlp` and setting `early_stopping=True`, `validation_fraction=0.3`. Keep all other parameters at their default values. This will create a classifier that automatically splits the original training set into nonoverlapping training and validation splits, where the validation split is 30% of the original training set.    \n",
    "\n",
    "#### Compare this classifier against the same model trained without early stopping.\n",
    "\n",
    "#### Fit each classifier on the training dataset and report its training and test accuracies.\n",
    "\n",
    "#### Also, plot the training loss and validation accuracy curves separately for the classifier trained with early stopping. Hint: use the validation_scores_ (analogous to loss_curve_) to plot the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "executionInfo": {
     "elapsed": 42922,
     "status": "ok",
     "timestamp": 1670362877450,
     "user": {
      "displayName": "Alexander La Cour",
      "userId": "15991973518995942312"
     },
     "user_tz": 300
    },
    "id": "zYeDKG6-uobx",
    "outputId": "c7c3d865-58b6-43b7-d57d-6fec1ec32de0"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLII4Ngn9pxq"
   },
   "source": [
    "#### Explain the plot and any change in the train and test performance compared to the classifier trained without early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC3G6PEy96lT"
   },
   "source": [
    "Insert answer"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nav_menu": {
   "height": "309px",
   "width": "468px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
