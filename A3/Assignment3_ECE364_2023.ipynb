{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_lm7Q-9VKMn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install libgraphviz-dev\n",
    "!pip install pomegranate==0.14.9 matplotlib pygraphviz==1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryOZJYQa3PG0"
   },
   "outputs": [],
   "source": [
    "random_state=5 # use this to control randomness across runs e.g., dataset partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BASGXrOy4wat"
   },
   "source": [
    "## Preparing the Credit Card Fraud Detection dataset (1 points)\n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "See [here](https://www.kaggle.com/mlg-ulb/creditcardfraud) for details of the dataset. We will post process the data to balance both the classes indicating whether the transaction is fraud or not.\n",
    "\n",
    "We will use a subset of the dataset for simplifying the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URgO9HCN6RCl"
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6600,
     "status": "ok",
     "timestamp": 1626659481683,
     "user": {
      "displayName": "Sayeri Lala",
      "photoUrl": "",
      "userId": "13028116931964754599"
     },
     "user_tz": 420
    },
    "id": "tlKBDyEQDzrk",
    "outputId": "411b31f5-3764-4fe5-ae76-54033f87195f"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "if not os.path.exists('creditcard.csv'): \n",
    "    !wget https://raw.githubusercontent.com/JHA-Lab/ece364_2023/master/dataset/creditcard.zip \n",
    "with ZipFile(\"creditcard.zip\", 'r') as zObject:\n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "    zObject.extractall(\n",
    "        path=\".\")\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "ind_subset=np.array([0,2,3,4,5,-2,-1])\n",
    "subset_features=df.columns[ind_subset]\n",
    "df=df[subset_features]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1626659481684,
     "user": {
      "displayName": "Sayeri Lala",
      "photoUrl": "",
      "userId": "13028116931964754599"
     },
     "user_tz": 420
    },
    "id": "Npvx1EvED1Lo",
    "outputId": "c3462c6b-b049-4bdd-eb3e-b86127bfb34d"
   },
   "outputs": [],
   "source": [
    "# Check the datatype of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8BtplCfD43h"
   },
   "source": [
    "#### There are a total of 284807 entries in this dataset with no missing values. The last column indicates whether the transaction is fraud or not while remaining columns are features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1626659482186,
     "user": {
      "displayName": "Sayeri Lala",
      "photoUrl": "",
      "userId": "13028116931964754599"
     },
     "user_tz": 420
    },
    "id": "npMqPm_kD7Va",
    "outputId": "5f8a3358-db59-4482-8e1a-9134cfa60e3e"
   },
   "outputs": [],
   "source": [
    "##### Look at some statistics of the data using the `describe` function\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D792olj-D_QL"
   },
   "source": [
    "#### Visualize the distribution of fraudulent vs genuine transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1626659482195,
     "user": {
      "displayName": "Sayeri Lala",
      "photoUrl": "",
      "userId": "13028116931964754599"
     },
     "user_tz": 420
    },
    "id": "njzVsqZKEEZy",
    "outputId": "aedf8cce-e54b-490b-8083-d4f37c0cfa6f"
   },
   "outputs": [],
   "source": [
    "# Make a pie chart showing transaction type\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.pie(df.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['green','red'])\n",
    "plt.axis('equal')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "executionInfo": {
     "elapsed": 950,
     "status": "ok",
     "timestamp": 1626659483129,
     "user": {
      "displayName": "Sayeri Lala",
      "photoUrl": "",
      "userId": "13028116931964754599"
     },
     "user_tz": 420
    },
    "id": "PDR0nrZiEGoW",
    "outputId": "2e78f0ef-c6a2-4b87-ad1b-a515657b079a"
   },
   "outputs": [],
   "source": [
    "## Check fradulent activity over time (note: total time os 48 hours)\n",
    "df[\"Time_Hr\"] = df[\"Time\"]/3600 # convert to hours\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex = True, figsize=(6,3))\n",
    "ax1.hist(df.Time_Hr[df.Class==0],bins=48,color='g',alpha=0.5)\n",
    "ax1.set_title('Genuine')\n",
    "ax2.hist(df.Time_Hr[df.Class==1],bins=48,color='r',alpha=0.5)\n",
    "ax2.set_title('Fraud')\n",
    "plt.xlabel('Time (hrs)')\n",
    "plt.ylabel('# transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64QUegPwEKdX"
   },
   "outputs": [],
   "source": [
    "# Remove 'Time' feature as it is already captured when converting to hours\n",
    "df = df.drop(['Time'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ax7Nd7LEQZx"
   },
   "source": [
    "#### Create a balanced dataset with 50% from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tg-zhzqZEMZI"
   },
   "outputs": [],
   "source": [
    "fraud_indices = np.array(df[df.Class == 1].index) #indices corresponding to fraud transaction\n",
    "genuine_ind = df[df.Class == 0].index #indices corresponding to genuine transaction\n",
    "total_fraud_transactions = len(df[df.Class == 1]) # total transactions that were fraud\n",
    "np.random.seed(0) # fix the random seed generator for consistent results\n",
    "indices_genuine_transaction = np.random.choice(genuine_ind, total_fraud_transactions, replace = False)\n",
    "indices_genuine_transaction = np.array(indices_genuine_transaction)\n",
    "selected_balanced_indices = np.concatenate([fraud_indices,indices_genuine_transaction]) # indices for balanced data\n",
    "balanced_data = df.iloc[selected_balanced_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1626659483480,
     "user": {
      "displayName": "Sayeri Lala",
      "photoUrl": "",
      "userId": "13028116931964754599"
     },
     "user_tz": 420
    },
    "id": "RydHugdIEWI4",
    "outputId": "6ff9aad1-ca80-43d6-f242-902b60f7bb50"
   },
   "outputs": [],
   "source": [
    "print(\"% genuine transactions: \",len(balanced_data[balanced_data.Class == 0])/len(balanced_data))\n",
    "print(\"% fraud transactions: \",len(balanced_data[balanced_data.Class == 1])/len(balanced_data))\n",
    "\n",
    "# Make a pie chart showing transaction type\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.pie(balanced_data.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['green','red'])\n",
    "plt.axis('equal')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRhEcln77rIK"
   },
   "source": [
    "### Extract target and descriptive features (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blhp_Upk8E-Y"
   },
   "outputs": [],
   "source": [
    "# Store all the features from the data in X\n",
    "X= # insert your code here\n",
    "# Store all the labels in y\n",
    "y= # insert your code here\n",
    "\n",
    "FEATURE_NAMES=X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdUFK3qG8Gnk"
   },
   "outputs": [],
   "source": [
    "# Convert data to numpy array\n",
    "X = # insert your code here\n",
    "y = # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-JPYSnc8JQi"
   },
   "source": [
    "### Create training and validation datasets (1 point)\n",
    "\n",
    "\n",
    "We will split the dataset into training and validation set. Generally in machine learning, we split the data into training,\n",
    "validation and test set (this will be covered in later chapters). The model with best performance on the validation set is used to evaluate perfromance on \n",
    "the test set which is the unseen data. In this assignment, we will using `train set` for training and evaluate the performance on the `test set` for various \n",
    "model configurations to determine the best hyperparameters (parameter setting yielding the best performance).\n",
    "\n",
    "Split the data into training and validation set using `train_test_split`.  See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set `random_state` to the value defined earlier. We use 80% of the data for training and 20% of the data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzTzI3iT8R5x"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training probability-based classifiers (18 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Learning a Naive Bayes Model (9 points)\n",
    "\n",
    "#### We will use the `pomegranate` library to train a Naive Bayes Model. Review ch.6 and see [here](https://pomegranate.readthedocs.io/en/stable/NaiveBayes.html) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate.distributions import NormalDistribution, ExponentialDistribution, UniformDistribution, DiscreteDistribution, PoissonDistribution \n",
    "from pomegranate.NaiveBayes import NaiveBayes\n",
    "from pomegranate.BayesClassifier import BayesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import math\n",
    "\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1a: Fit naive bayes model using a single distribution type (2 points)\n",
    "\n",
    "#### Train one naive bayes model using a normal distribution for each feature. Train another naive bayes model using an exponential distribution for each feature. Hint: use NormalDistribution or ExponentialDistribution (continuous features) and NaiveBayes.from_samples() to fit the model to the data.\n",
    "\n",
    "#### Report the training and test set accuracies for each model. Hint: use accuracy_score()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1b: Fit a naive bayes model using different feature distributions (3 points)\n",
    "\n",
    "#### Visualize the feature distributions (done for you below) to determine which distribution (normal or exponential) better models a specific feature. \n",
    "\n",
    "#### Train a Naive Bayes classifier using this set of feature-specific distributions. Hint: use NormalDistribution or ExponentialDistribution and NaiveBayes.from_samples() to fit the model to the data.\n",
    "\n",
    "#### Report the training and test set accuracies for the model. Hint: use accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization code\n",
    "\n",
    "num_cols=4\n",
    "num_rows=int(len(FEATURE_NAMES)/num_cols) if len(FEATURE_NAMES)%num_cols == 0 else int(math.ceil(len(FEATURE_NAMES)/num_cols))\n",
    "fig,ax=plt.subplots(num_rows,num_cols,figsize=(10,10))\n",
    "\n",
    "for ft_index in np.arange(X_train.shape[1]):\n",
    "    ax[ft_index//num_cols,ft_index%num_cols].hist(X_train[:,ft_index], color='blue')\n",
    "    ax[ft_index//num_cols,ft_index%num_cols].hist(X_test[:,ft_index], color='red')\n",
    "    ax[ft_index//num_cols,ft_index%num_cols].set_title(FEATURE_NAMES[ft_index])\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment on any performance difference between this model and the models trained in Ex. 1a. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1c: Fit a naive bayes model on categorical features (2 points)\n",
    "\n",
    "#### Besides fitting a naive bayes model on the continuous features, one can fit a naive bayes model on categorical features derived from binning the continuous features, and then compute a probability mass function for each categorical feature.\n",
    "\n",
    "#### Bin the features by varying the strategy among {equal-width binning, equal-frequency binning}. For each binning strategy, vary the number of bins among {3,20,50}. Hint: use [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer.get_params) by modifying n_bins and strategy and setting encode=\"ordinal\" to map the labels to numerical categories.\n",
    "\n",
    "#### For each binning setting tried above, fit a naive bayes model on the binned version of the training set. Hint: use DiscreteDistribution to model the categorical features and NaiveBayes.from_samples() to fit the model to the data.\n",
    "\n",
    "#### Report the training and test set accuracy for each model trained and evaluated on binned versions of the training and test sets respectively. \n",
    "\n",
    "**Note** There may be some variability in the actual performance scores, but the overall trends should remain the consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Briefly explain any performance difference between equal-width and equal-frequency binning. Also comment on the effect of increasing the number of bins (see ch.3). (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Learning a Bayes Net (9 points)\n",
    "\n",
    "#### We will use the `pomegranate` library to train a Bayes Net to assess whether relaxing the assumption in Naive bayes (i.e., all features are independent given the target feature) could improve the classification model. Review ch.6 and see [here](https://pomegranate.readthedocs.io/en/stable/BayesianNetwork.html) for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2a: Create a categorical version of the dataset (1 point)\n",
    "\n",
    "#### Create categorical versions of the training and test sets by using equal-frequency binning with the number of bins set to 20 (as in Ex. 1c). \n",
    "\n",
    "#### *<u>Use these datasets for training and evaluating the bayes net models in the following exercises.<u>*\n",
    "\n",
    "**Note** This is done because pomegranate currently only supports bayes net over categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2b: Construct a Bayes net (3 points)\n",
    "\n",
    "#### Construct and train a Bayes net in which the V2 node is a parent of the target feature node (only these 2 nodes should be in the net). Use construct_and_train_bayes_net (defined below) by passing in the binned training dataset and specifying the index of the parent feature node.\n",
    "\n",
    "#### Construct and train another Bayes net in which the Time_hr node is a parent of the target feature node (only these 2 nodes should be in the net). Use construct_and_train_bayes_net (defined below) by passing in the binned training dataset and specifying the index of the parent feature node.\n",
    "\n",
    "#### Report the training and test accuracies of each Bayes Net. Use get_performance (defined below) by passing in the trained bayes net, binned datasets, and specifying the index of the parent feature node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate import *\n",
    "\n",
    "\"\"\"\n",
    "X_train_binned: ndarray (# instances, # features) This is the binned version of the training set\n",
    "y_train: 1darray (# instances,)\n",
    "ind_chosen_parent_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n",
    "                            These indices correspond to features that are parent nodes of the target node. \n",
    "ind_chosen_child_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n",
    "                            These indices correspond to features that are children nodes of the target node.\n",
    "                            \n",
    "Returns a BayesianNetwork representing the trained bayes net\n",
    "\"\"\"\n",
    "def construct_and_train_bayes_net(X_train_binned,\n",
    "                                  y_train,\n",
    "                                  ind_chosen_parent_features=np.array([]), \n",
    "                                  ind_chosen_child_features=np.array([]),\n",
    "                                ):\n",
    "    # parent nodes of target\n",
    "\n",
    "    dist_by_parent_feature=[]\n",
    "    state_by_parent_feature=[]\n",
    "    if len(ind_chosen_parent_features)>0:\n",
    "        parent_feature_names_chosen=FEATURE_NAMES[ind_chosen_parent_features]\n",
    "\n",
    "        for ft_index in ind_chosen_parent_features:\n",
    "            ft_dist=DiscreteDistribution.from_samples(X_train_binned[:,ft_index])\n",
    "            dist_by_parent_feature.append(ft_dist)\n",
    "            state_by_parent_feature.append(State(ft_dist, str(FEATURE_NAMES[ft_index])))\n",
    "        dist_by_parent_feature=np.array(dist_by_parent_feature)\n",
    "        state_by_parent_feature=np.array(state_by_parent_feature)\n",
    "\n",
    "\n",
    "    # target node\n",
    "    if len(ind_chosen_parent_features)>0:\n",
    "        X_train_parent_features_binned_with_labels=np.concatenate((X_train_binned[:,ind_chosen_parent_features],\n",
    "                                                                   np.expand_dims(y_train,axis=1)),axis=1)\n",
    "        target_dist=ConditionalProbabilityTable.from_samples(X_train_parent_features_binned_with_labels)\n",
    "        # temporary workaround to properly initialize the distribution\n",
    "        target_dist=ConditionalProbabilityTable(target_dist.parameters[0],dist_by_parent_feature.tolist())\n",
    "    else:\n",
    "        target_dist=DiscreteDistribution.from_samples(y_train)\n",
    "    target_state=State(target_dist, \"target\")\n",
    "\n",
    "    # children node of target\n",
    "\n",
    "    dist_by_child_feature=[]\n",
    "    state_by_child_feature=[]    \n",
    "    if len(ind_chosen_child_features)>0:\n",
    "        child_feature_names_chosen=FEATURE_NAMES[ind_chosen_child_features]\n",
    "\n",
    "        for ft_index in ind_chosen_child_features:\n",
    "            X_train_child_features_binned_with_labels=np.concatenate((np.expand_dims(y_train,axis=1),\n",
    "                                                                        np.expand_dims(X_train_binned[:,ft_index],axis=1)),\n",
    "                                                                     axis=1)\n",
    "            ft_dist=ConditionalProbabilityTable.from_samples(X_train_child_features_binned_with_labels)\n",
    "            ft_dist=ConditionalProbabilityTable(ft_dist.parameters[0],[target_dist])\n",
    "            dist_by_child_feature.append(ft_dist)\n",
    "            state_by_child_feature.append(State(ft_dist, str(FEATURE_NAMES[ft_index])))\n",
    "        dist_by_child_feature=np.array(dist_by_child_feature)\n",
    "        state_by_child_feature=np.array(state_by_child_feature)\n",
    "\n",
    "\n",
    "    pom_model = BayesianNetwork()\n",
    "    pom_model.add_states(*list(state_by_parent_feature))\n",
    "    pom_model.add_states(target_state)\n",
    "    pom_model.add_states(*list(state_by_child_feature))\n",
    "\n",
    "    for parent_index in np.arange(len(ind_chosen_parent_features)):\n",
    "        pom_model.add_edge(state_by_parent_feature[parent_index],target_state)\n",
    "\n",
    "    for child_index in np.arange(len(ind_chosen_child_features)):\n",
    "        pom_model.add_edge(target_state, state_by_child_feature[child_index])\n",
    "\n",
    "    pom_model.bake()\n",
    "\n",
    "    return pom_model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pom_model: BayesianNetwork represents the trained bayes net model\n",
    "X_train_binned: ndarray (# instances, # features) This is the binned training set\n",
    "y_train: 1darray (# instances,)\n",
    "X_test_binned: ndarray (# instances, # features) This is the binned test set\n",
    "y_test: 1darray (# instances,)\n",
    "ind_chosen_parent_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n",
    "                            These indices correspond to features that are parent nodes of the target node. \n",
    "ind_chosen_child_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n",
    "                            These indices correspond to features that are children nodes of the target node.\n",
    "                            \n",
    "Returns the training and test set accuracies attained by the bayes net model (pom_model)\n",
    "\"\"\"\n",
    "def get_performance(pom_model, X_train_binned, y_train, X_test_binned, y_test, \n",
    "                    ind_chosen_parent_features=np.array([]), ind_chosen_child_features=np.array([])):\n",
    "    nones_array=np.expand_dims(np.array([None]*len(X_train_binned)),axis=1)\n",
    "    ind_target_node=len(ind_chosen_parent_features)\n",
    "    if len(ind_chosen_parent_features)>0:\n",
    "        X_train_binned_with_none=X_train_binned[:,ind_chosen_parent_features]\n",
    "        X_train_binned_with_none=np.concatenate((X_train_binned_with_none,nones_array),axis=1)\n",
    "    else:\n",
    "        X_train_binned_with_none=nones_array\n",
    "\n",
    "    if len(ind_chosen_child_features)>0:\n",
    "        X_train_binned_with_none=np.concatenate((X_train_binned_with_none,\n",
    "                                                X_train_binned[:,ind_chosen_child_features]),\n",
    "                                               axis=1)\n",
    "    pred_labels=np.array(pom_model.predict(X_train_binned_with_none),dtype='int64')[:,ind_target_node]\n",
    "    train_acc=accuracy_score(y_train, pred_labels)\n",
    "\n",
    "    nones_array=np.expand_dims(np.array([None]*len(X_test_binned)),axis=1)\n",
    "    if len(ind_chosen_parent_features)>0:\n",
    "        X_test_binned_with_none=X_test_binned[:,ind_chosen_parent_features]\n",
    "        X_test_binned_with_none=np.concatenate((X_test_binned_with_none,nones_array),axis=1)\n",
    "    else:\n",
    "        X_test_binned_with_none=nones_array\n",
    "\n",
    "    if len(ind_chosen_child_features)>0:\n",
    "        X_test_binned_with_none=np.concatenate((X_test_binned_with_none,\n",
    "                                               X_test_binned[:,ind_chosen_child_features]),\n",
    "                                               axis=1)\n",
    "    pred_labels=np.array(pom_model.predict(X_test_binned_with_none),dtype='int64')[:,ind_target_node]\n",
    "    test_acc=accuracy_score(y_test, pred_labels)\n",
    "    \n",
    "    return train_acc, test_acc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here (for the case where V2 is the parent node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here (for the case where Time_hr is the parent node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment on which feature seems more informative for the prediction task. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2c: Construct a Bayes net with multiple parent nodes (3 points)\n",
    "\n",
    "#### Construct and train a Bayes net in which:\n",
    "#### -the following features are all parents of the target feature node (V2, V3, Time_Hr).  \n",
    "#### Use construct_and_train_bayes_net by passing in the binned training dataset and specifying the indices of the parent feature nodes. (this could take several minutes)\n",
    "\n",
    "#### Report the training and test accuracy of the Bayes Net using get_performance by passing in the trained bayes net, binned datasets, and indices of the parent feature nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the performance of this Bayes net against the Bayes nets from Ex. 2b. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert answer"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nav_menu": {
   "height": "309px",
   "width": "468px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
