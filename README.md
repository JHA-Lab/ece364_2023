# ECE 364: Machine Learning for Predictive Data Analytics

[![License](https://img.shields.io/badge/License-BSD%203--Clause-red.svg)](https://github.com/JHA-Lab/ece364/blob/main/LICENSE)
![Python 3.7, 3.8](https://img.shields.io/badge/python-3.7%20%7C%203.8-blue.svg)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FJHA-Lab%2Fece364&count_bg=%23FFC401&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)

Welcome to ECE 364! In this course we present basic concepts in machine learning for predictive data analytics:
* **Information-based learning:** decision trees; Shannon’s entropy model; information gain; ID3 algorithm; feature selection; predicting continuous targets; tree pruning; model ensembles: boosting, bagging.
* **Similarity-based learning:** feature spac, distance metrics, nearest neighbor algorithm, noisy data, predicting continuous targets, various measures of similarity, feature selection.
* **Probability-based learning:** Bayes' theorem; Bayesian prediction; curse of dimensionality; conditional independence and factorization; Naive Bayes model; smoothing; continuous features: probability density functions, binning; Bayesian networks. 
* **Error-based learning:**  simple linear regression; error surface; multivariate linear regression; gradient descent; learning rate; handling categorical features; modeling nonlinear relationships; multinomial logistic regression; support vector machines.
* **Deep learning:** artificial neural networks; activation functions; backpropagation and gradient descent; vanishing gradients; weight initialization; categorical target features: softmax layer and cross-entropy loss; dropout.
* **Evaluation:** misclassification rate; confusion matrix: precision, recall, F1 measure, profit/loss; prediction scores: receiver operating characteristic curve, Kolmogorov-Smirnov statistic, gain/lift; measures of error; evaluating models after deployment.
* **The _art_ of machine learning:** perspectives on prediction models: parametric vs. nonparametric, generative vs. discriminative; choosing a machine learning approach: no free lunch theorem, project requirements, data considerations.

## Useful Links

* Canvas [web site](https://canvas.princeton.edu/)
* Zoom links and TA contact information can be found on Canvas.

## Course staff

* Instructor: [Niraj K. Jha](https://www.princeton.edu/~jha/)
* TAs: Sayeri Lala, Margarita Belova

## Timings

* Lectures: M/W 3:00-4:20pm (EQuad B205)
* Office hours:
    * Niraj K. Jha: M/W 2-3pm (EQuad B205)
    * Sayeri Lala: Tu: 4-5pm, Th: 11-12pm (EQuad B321)
    * Margarita Belova: M: 11-12pm, W:1-2pm (EQuad B321)

 ## Grading

 * Assignments (25%):
 * Mid-term exam (25%)
 * Final exam (50%)

## Reading

* [John D. Kelleher, Brian Mac Namee, and Aoife D’Arcy, Fundamentals of Machine Learning for Predictive Data Analytics, 2nd edition, The MIT Press](https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics-second-edition)

## License

BSD-3-Clause. 
Copyright (c) 2023, JHA-Lab.
All rights reserved.

See License file for more details.
