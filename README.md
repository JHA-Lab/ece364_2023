# ECE 364: Machine Learning for Predictive Data Analytics

[![License](https://img.shields.io/badge/License-BSD%203--Clause-red.svg)](https://github.com/JHA-Lab/ece364/blob/main/LICENSE)
![Python 3.7, 3.8](https://img.shields.io/badge/python-3.7%20%7C%203.8-blue.svg)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FJHA-Lab%2Fece364&count_bg=%23FFC401&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)

Welcome to ECE 364! In this course we present basic concepts in machine learning for predictive data analytics:
* **Information-based learning:** decision trees; Shannon’s entropy model; information gain; ID3 algorithm; feature selection; predicting continuous targets; tree pruning; model ensembles: boosting, bagging.
* **Similarity-based learning:** feature spac, distance metrics, nearest neighbor algorithm, noisy data, predicting continuous targets, various measures of similarity, feature selection.
* **Probability-based learning:** Bayes' theorem; Bayesian prediction; curse of dimensionality; conditional independence and factorization; Naive Bayes model; smoothing; continuous features: probability density functions, binning; Bayesian networks. 
* **Error-based learning:**  simple linear regression; error surface; multivariate linear regression; gradient descent; learning rate; handling categorical features; modeling nonlinear relationships; multinomial logistic regression; support vector machines.
* **Deep learning:** artificial neural networks; activation functions; backpropagation and gradient descent; vanishing gradients; weight initialization; categorical target features: softmax layer and cross-entropy loss; dropout.
* **Evaluation:** misclassification rate; confusion matrix: precision, recall, F1 measure, profit/loss; prediction scores: receiver operating characteristic curve, Kolmogorov-Smirnov statistic, gain/lift; measures of error; evaluating models after deployment.
* **The _art_ of machine learning:** perspectives on prediction models: parametric vs. nonparametric, generative vs. discriminative; choosing a machine learning approach: no free lunch theorem, project requirements, data considerations.

## Useful Links

* Canvas [web site](https://princeton.instructure.com/courses/7820)
* Zoom links and TA contact information can be found on Canvas.
* Other useful docs:
    * [Assignment Logistics](https://docs.google.com/viewer?url=https://github.com/JHA-Lab/ece364/raw/main/Assignment%20Logistics.pdf)
    * [Working with Google Colab](https://docs.google.com/viewer?url=https://github.com/JHA-Lab/ece364/raw/main/Working%20with%20Google%20Colab.pdf)
    * [Working with LaTeX](https://docs.google.com/viewer?url=https://github.com/JHA-Lab/ece364/raw/main/Working%20with%20LaTeX.pdf)

## Course staff

* Instructor: [Niraj K. Jha](https://www.princeton.edu/~jha/)
* TAs: Hongjie Wang, Alexander La Cour

## Timings

* Lectures: Tue/Thurs 3:00-4:20pm (EQuad B205)
* Office hours:
    * Niraj K. Jha: Tue/Thurs 2:00-3:00pm
    * Hongjie Wang: Tue/Thurs 11:00am-12:00pm
    * Alexander La Cour: Mon/Wed 2:00pm-3:00pm

 ## Grading

 * Assignments (25%):
     * A0: no grade
     * A1-A5: written theory portion (40 pts), Google Colab programming portion (20 pts)
     * A6: written theory portion (40 pts)
 * Mid-term exam (25%)
 * Final exam (50%)

## Reading

* [John D. Kelleher, Brian Mac Namee, and Aoife D’Arcy, Fundamentals of Machine Learning for Predictive Data Analytics, 2nd edition, The MIT Press](https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics-second-edition)

## License

BSD-3-Clause. 
Copyright (c) 2022, JHA-Lab.
All rights reserved.

See License file for more details.
